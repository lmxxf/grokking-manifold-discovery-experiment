# Grokking 作为流形发现：延迟泛化的几何重解释

**作者**：Jin Yanyan (lmxxf@hotmail.com), Zhao Lei (zhaosanshi@gmail.com)

**摘要**：Grokking——神经网络在长期过拟合后突然泛化的现象——自 2022 年被发现以来，已积累了多种理论解释：Goldilocks Zone、Softmax Collapse、Lazy-Rich 过渡等。本文综述这些理论，指出它们的共同盲区：**大多侧重外部测量，缺乏对表示空间几何结构的直接刻画**。其中 Goldilocks Zone 理论触及了高维空间的"物理法则"，具有较高的理论价值。我们提出一个统一框架——**流形发现假说**：记忆是穿过所有训练点的高维锯齿曲线，泛化是发现数据分布的低维流形，Grokking 是从前者到后者的相变。**我们在模加法和模乘法两组实验上给出支持该假说的证据**：观察到表示的有效维度显著下降（PCA 95% 口径下 78→8 / 89→11）、拓扑摘要量级变化、以及降维可视化中簇结构的涌现。特别地，模乘法实验发现模型学到了商群结构（$k \mod 12$ 的陪集，纯度 99.4%），这促使我们将假说修正为**两阶段模型**：局部流形发现 → 全局粘合。一句话概括：**高维曲线 → 低维曲面**。

---

## 1. 引言：Grokking 为什么重要

2022 年，Power 等人在 OpenAI 发现了一个反直觉的现象：在模运算任务上训练的小型 Transformer，会先**完美过拟合**训练集（训练 loss 降到零、测试 accuracy 接近随机），然后在**几万甚至几十万步之后**，测试 accuracy 突然从随机水平跳升到接近 100%。

他们把这个现象命名为 **Grokking**（顿悟）。

这个发现之所以重要，是因为它挑战了深度学习的核心假设：

1. **经典假设**：过拟合是泛化的敌人，一旦过拟合就应该早停
2. **Grokking 反例**：过拟合可以持续很长时间，然后突然泛化

如果泛化真的可以在过拟合之后发生，那"早停"策略可能杀死了很多本可以泛化的模型。

更深层的问题：**Grokking 时模型内部发生了什么？**

过去三年，学术界积累了多种理论解释。本文的任务是综述这些理论，指出它们的盲区，并提出一个统一框架。

---

## 2. 现有理论综述

### 2.1 Goldilocks Zone 理论（Liu et al. 2022）

**核心观点**：权重范数需要落在一个"刚刚好"的区间内，才能泛化。

Liu 等人在 NeurIPS 2022 发现，权重空间中存在一个**空心球壳**，他们称之为 Goldilocks Zone（金发姑娘区）：

- 半径太大（$\|w\| > w_c$）：过拟合，记住训练集
- 半径太小（$\|w\| < w_c$）：欠拟合，什么都学不会
- 刚好在壳上（$\|w\| \approx w_c$）：泛化

**Grokking 的发生机制**：
1. 大初始化把模型放在球壳外面
2. 模型先快速过拟合（训练 loss 降到零）
3. 权重衰减缓慢地把权重范数拉回 Goldilocks Zone
4. 一旦进入球壳 → 突然泛化 → Grokking

**这篇论文的真正价值**：它暗示了高维空间有自己的"物理法则"——权重衰减是引力，Goldilocks Zone 是稳定轨道。这是后续所有理论的基础。

**局限**：描述了"在哪儿泛化"，没解释"为什么那儿能泛化"。Goldilocks Zone 是什么的代理变量？

### 2.2 Softmax Collapse 理论（Prieto et al. 2025）

**核心观点**：没有权重衰减，Grokking 会被浮点数精度杀死。

模型为了降低交叉熵 loss，会疯狂放大正确答案的 logit（比如正确类 = 1000，其他类 = 1）。Softmax 计算时 $e^{1000}$ 直接溢出，梯度归零，训练卡死。

**权重衰减的作用**：持续把权重往回拉，防止 logit 无限增长，保持梯度存活。

**替代方案**：论文提出 StableMax + 垂直梯度（阻止梯度往"放大 logit"方向走），可以不用权重衰减也触发 Grokking。不过这种方法的收敛效率可能较低——权重衰减是全局压缩，作用范围广；垂直梯度是局部约束，作用范围窄。实际工程中权重衰减仍是更常用的选择。

**贡献**：解释了"没有权重衰减会怎样"。

**局限**：只解释了"为什么训练不会停"，没解释"为什么最终会泛化"。

### 2.3 Lazy → Rich 过渡理论（Kumar et al. 2024）

**核心观点**：Grokking 是从 lazy training 到 feature learning 的相变。

借用了神经正切核（NTK）的语言：

- **Lazy regime**：权重几乎不动，模型像线性分类器
- **Rich regime**：权重大幅调整，学到真正的非线性特征

Grokking 发生在 lazy → rich 的**相变点**。

**争议**：这派人声称，在特定条件下（浅层网络 + MSE loss），不需要权重衰减也能触发 Grokking。

**评价**：这一理论的主要贡献是引入了 lazy/rich 的概念框架，但在解释"为什么相变会发生"方面仍有空间。

### 2.4 权重效率假说（Varma et al. 2023）

**核心观点**：权重衰减偏好"权重更小"的解，而泛化解通常比记忆解更权重高效。

- 记忆解：需要大量权重来硬记每个样本
- 泛化解：用简洁的规则覆盖所有样本，权重更小
- 权重衰减 → 惩罚大权重 → 偏好泛化解

**评价**：与 2.3 类似，这一理论提供了一个有用的视角（小权重 ↔ 泛化），但本质上是对同一现象的另一种描述，尚未揭示因果机制。

### 2.5 Mechanistic Interpretability 视角（Nanda et al. 2023）

Nanda 等人在 ICLR 2023 (Oral) 做了一项扎实的工作：**完全逆向工程**了模型学到的算法。

**核心发现**：模运算 $(a + b) \mod p$ 的本质是一个**循环群**——0, 1, 2, ..., p-1 首尾相连，形成一个离散的环。模型把这个模运算解构成了傅里叶级数。人类事后分析权重矩阵发现，它等价于傅里叶变换的结构。模型自己只是在做矩阵乘法，根本不知道什么傅里叶公式。

**讨论**：模运算天然是周期的，用傅里叶级数展开是 200 年前就有的数学工具。从这个角度看，模型"发现"傅里叶结构更像是对任务内在周期性的必然响应，而非意外发现。

**贡献**：苦力活，把模型拆开看了。

**局限**：没解释为什么 weight decay + 过拟合 + 继续训练 = 傅里叶变换。

### 2.6 现有理论的共同盲区

| 理论 | 问的问题 | 没问的问题 |
|------|---------|-----------|
| Goldilocks Zone | 权重范数在哪个区间 | 那个区间有什么特别 |
| Softmax Collapse | 为什么训练不会停 | 为什么最终会泛化 |
| Lazy → Rich | 权重怎么变化 | 表示怎么变化 |
| 权重效率 | 哪个解权重更小 | 为什么小权重 = 泛化 |
| Mechanistic Interp. | 学到了什么电路 | 为什么是这个电路 |

**评价**：Goldilocks Zone 理论触及了高维空间的"物理法则"，具有较高的理论价值。Softmax Collapse、Lazy→Rich、权重效率等理论各自提供了有用的视角，但主要停留在外部测量层面（权重范数、梯度大小、Loss 曲线），尚未揭示表示空间的几何本质。Mechanistic Interpretability 开始从内部看，但侧重具体电路，不是几何结构。

总体而言，现有理论在"描述现象"方面做了大量工作，但在"解释机制"方面仍有提升空间。本文尝试从几何视角提供一个补充性的统一框架。

---

## 3. 统一框架：流形发现假说

我们提出一个统一框架：**Grokking 是从高维锯齿曲线到低维平滑流形的转变（可能伴随临界态震荡）。**

### 3.1 记忆 vs 泛化的几何解释

**记忆 = 锯齿曲线**

当模型过拟合训练集时，它用一条复杂的锯齿曲线穿过每一个训练样本点。这条曲线能精准命中所有训练数据，但它**没有规律**——只是硬把所有点串起来，点与点之间没有结构关系。

**泛化 = 流形发现**

当模型真正"理解"任务时，它发现训练样本其实分布在一个**低维流形**上（低维是相对于模型的 hidden dim 而言）。

以模运算 $a + b \mod p$ 为例：
- 输入空间是 $p^2$ 个离散点
- 但输出只取决于 $(a + b) \mod p$，即**同余类**
- 真正的结构是一个一维的**循环群** $\mathbb{Z}_p$（只有一个自由度：位置）

泛化意味着：模型发现了这个循环群结构，而不是硬记 $p^2$ 个输入-输出对。

**Grokking = 从曲线到流形的相变**

Grokking 发生的事：
1. 之前：表示空间里是一条穿过所有点的高维锯齿曲线（记忆解）
2. 之后：曲线坍缩到一个低维流形上，流形的拓扑结构对应任务的真正结构（泛化解）

这是一个**多稳态系统的相变**——记忆解和泛化解是两个稳定状态，模型在训练过程中从前者跃迁到后者。但需要注意：实验观察到的并非单次干净相变，而是**临界态竞争**——模型可能在两种解之间反复跳跃（见 6.2.3 节的震荡现象），最终才稳定在泛化解上。

**一句话概括：高维曲线 → 低维曲面。**

### 3.2 权重衰减的几何作用

在这个框架下，权重衰减的作用变得清晰：

**权重衰减 = 让锯齿曲线不稳定的向心力**

没有权重衰减时：
- 梯度下降会把模型推向"loss 最低"的地方
- 对于过参数化的模型，这个地方是"完美记住每个训练样本"
- 模型会在锯齿曲线构成的解上稳定下来

有权重衰减时：
- 有一个与 loss 梯度相反的力，持续把权重往"更小"的方向拉
- 锯齿曲线需要"大权重"来维持（每个拐点需要专门的神经元）
- 平滑流形需要"小权重"（结构共享）
- 权重衰减**偏好流形编码**

**Goldilocks Zone 的真正含义**

Goldilocks Zone 不是"某个权重范数区间"，而是**能感知流形结构的激活模式**。

- 权重太大：锯齿曲线稳定，看不到流形
- 权重太小：信号太弱，什么结构都看不到
- 刚刚好：锯齿曲线不稳定 + 信号够强 → 流形涌现

### 3.3 Softmax Collapse 的几何解释

Softmax Collapse 的直接触发因素是数值稳定性（logit 过大导致溢出/下溢/梯度病态）。在本文框架下，它也可以被理解为一种“表示/决策过度单向化”的坍缩：某个方向被无限放大后，其他方向的可分辨信号被淹没，探索被提前终止。

当某个方向被过度强化时：
- 那个方向的 logit 趋向无穷大
- 其他方向的"声音"被淹没
- 模型失去了探索其他可能性的能力

**这就是为什么 Softmax Collapse 会杀死 Grokking**：发现流形需要同时"看到"多个方向，而坍缩把视野缩小到一个点。

权重衰减的作用：保持多方向的信号平衡，让模型能继续探索。

### 3.4 Lazy → Rich 的几何解释

Lazy → Rich 过渡不是关于权重动态，而是关于**表示复杂度**。

- Lazy regime：表示是输入的线性函数，只能编码线性可分的结构
- Rich regime：表示是输入的非线性函数，可以编码任意流形

**Grokking 需要 Rich regime**：因为真实任务的结构（如循环群）是非线性的。

为什么 Grokking 往往发生在训练后期？因为进入 Rich regime 需要权重有足够的变化，而初始化时模型处于 Lazy regime。

### 3.5 权重效率的几何解释

"小权重 = 泛化"的因果链：

1. 平滑流形比锯齿曲线更"紧凑"（维度更低）
2. 紧凑的编码需要更少的权重来实现
3. Weight Decay 偏好小权重 → 偏好紧凑编码 → 偏好流形

权重效率是流形发现的**结果**，不是**原因**。

### 3.6 一组可补齐的数学陈述（把"隐喻"落到可证的命题）

本节不试图"证明 Grokking 必然发生"（那需要对网络、数据分布、优化动力学做非常强的假设），而是把本文框架里最关键的几条直觉，改写成**在标准假设下可证明/可核查**的数学命题：

**关于 AdamW 与 L2 正则的说明**：以下推导基于标准梯度下降 + L2 正则。实验中使用的 AdamW 是**解耦权重衰减**（decoupled weight decay），其典型更新形式可写为
$$
w_{t+1} = (1-\eta\lambda)w_t - \eta \cdot \mathrm{Adam}(\nabla\mathcal{L}(w_t)),
$$
与“优化 $\mathcal{L} + \frac{\lambda}{2}\|w\|^2$”在自适应学习率下不完全等价。因此，本节的数学陈述应理解为**理论桥梁**——它们刻画了权重衰减的定性效应（向心力、小权重偏置），但不能直接套用到 AdamW 的精确动力学。

1) **Weight decay 的动力学形式**（它确实是一个向心力）；  
2) **$L_2$ 正则的最小范数偏置**（它确实偏好“共享结构/低复杂度”的插值解）；  
3) **“流形/群结构被发现”时内在维度为什么会降**（至少在“表示仅依赖于群不变量”的情况下是可严格推出的）。

> 记号：参数为 $w\in\mathbb{R}^m$，经验损失为 $\mathcal{L}(w)=\frac1n\sum_{i=1}^n \ell(f_w(x_i),y_i)$，weight decay 系数为 $\lambda\ge 0$，学习率为 $\eta>0$。

**引理 3.1（Weight decay = 向心力的离散动力学）**  
考虑带 $L_2$ 正则的目标
$$
J(w)=\mathcal{L}(w)+\frac{\lambda}{2}\|w\|_2^2.
$$
对 $J$ 做梯度下降：
$$
w_{t+1}=w_t-\eta\nabla J(w_t)= (1-\eta\lambda)w_t-\eta\nabla\mathcal{L}(w_t).
$$
因此，优化更新由两部分叠加：一部分把 $w_t$ 按比例缩小（向原点收缩），另一部分沿损失梯度下降。  
**证明**：直接展开 $\nabla(\frac{\lambda}{2}\|w\|^2)=\lambda w$ 即得。∎

这条引理把第 3.2 节的“向心力”从隐喻变成了明确的动力学项：在所有步上持续存在、与具体数据无关。

**命题 3.2（线性回归：正则化插值解趋向最小范数解）**  
设 $f_w(x)=w^\top x$，平方损失 $\ell=\frac12(f_w(x)-y)^2$，数据矩阵 $X\in\mathbb{R}^{n\times d}$ 满行秩，记 $y\in\mathbb{R}^n$。带 $L_2$ 正则的最优解（岭回归）满足
$$
w_\lambda = \arg\min_w \frac{1}{2n}\|Xw-y\|_2^2+\frac{\lambda}{2}\|w\|_2^2
      = (X^\top X+n\lambda I)^{-1}X^\top y.
$$
若存在插值解（即 $\exists w: Xw=y$），则当 $\lambda\downarrow 0$ 时，$w_\lambda$ 收敛到**最小 $L_2$ 范数的插值解**
$$
w_*=\arg\min_{Xw=y}\|w\|_2,
$$
且 $w_*$ 可写为 $w_*=X^\top(XX^\top)^{-1}y$。  
**证明（要点）**：用一阶最优条件得闭式解。对 $\lambda\to 0$，利用 $X$ 满行秩保证 $XX^\top$ 可逆，并用伪逆极限或等价的拉格朗日乘子推导可得。∎

这条命题在最简单的凸情形里明确了一件事：**$L_2$ 正则不只是“防过拟合”，它在插值可行时选择了“范数最小”的那个解**。把“范数”换成更一般的函数空间范数（如 RKHS/ Sobolev），就会自然得到“更平滑/更低频”的偏置——这与本文的“曲线（高频）→流形（低频）”直觉是一致的。

**命题 3.3（可分分类：无正则时权重范数趋于无穷大，有正则时最优解有界）**  
设二分类线性模型 $f_w(x)=w^\top x$，对数损失 $\ell(w;x,y)=\log(1+\exp(-y\,w^\top x))$，且数据线性可分：$\exists \bar w$ 使得对所有 $i$，$y_i\,\bar w^\top x_i>0$。则：
1) 无 $L_2$ 正则（$\lambda=0$）时，最小经验损失的下确界为 $0$，但一般**不存在有限范数的最优解**（可以沿着可分方向放大 $w$ 使损失任意接近 0）。  
2) 加入 $L_2$ 正则（$\lambda>0$）后，目标 $J(w)=\frac1n\sum_i \ell(w;x_i,y_i)+\frac{\lambda}{2}\|w\|^2$ 在 $\mathbb{R}^d$ 上是**强凸 + 下半连续**，从而存在唯一最优解 $w_\lambda$，且 $\|w_\lambda\|<\infty$。  
**证明（要点）**：1) 用 $\ell(\alpha w)\to 0$（$\alpha\to\infty$）说明可以把损失压到任意小但不达成。2) $L_2$ 项给出强凸与强制性（coercive）：$\|w\|\to\infty$ 时 $J(w)\to\infty$，从而存在唯一极小点。∎

这条命题对应第 2.2 节的 Softmax/Logit 爆炸现象：在交叉熵/对数损失里，“继续变好”的捷径往往是把 margin 放大到无穷；weight decay 把它变成一个有界的优化问题，梯度不会因为“范数无穷大”这种逃逸而自然消失。

**命题 3.4（“发现群结构” ⇒ 表示自由度下降：一个严格可检验的充分条件）**  
以模加为例，输入为 $(a,b)\in\mathbb{Z}_p^2$，令和为 $s=a+b\ (\mathrm{mod}\ p)\in\mathbb{Z}_p$。设某层表示为 $h(a,b)\in\mathbb{R}^k$，且存在函数 $\phi:\mathbb{Z}_p\to\mathbb{R}^k$ 使得
$$
h(a,b)=\phi(s)\quad\text{只依赖于 }s=(a+b)\bmod p.
$$
则表示集合 $\{h(a,b):a,b\in\mathbb{Z}_p\}$ 至多包含 $p$ 个点（恰为 $\phi(\mathbb{Z}_p)$），其有效自由度不再与 $p^2$ 成正比，而被 $p$ 上界控制。进一步地，若存在一条光滑嵌入 $\Phi:S^1\to\mathbb{R}^k$ 与一个同态 $\iota:\mathbb{Z}_p\hookrightarrow S^1$（把离散循环群嵌入圆周）使得 $\phi=\Phi\circ \iota$，则这些表示点落在一个一维流形（圆周）上。  
**证明**：第一部分由“函数只依赖 $s$”立即推出：不同 $(a,b)$ 只要同余类相同就映射到同一个 $h$，因而不同表示的数量被同余类数 $p$ 上界。第二部分是直接代入复合映射定义：$\phi(\mathbb{Z}_p)\subseteq \Phi(S^1)$。∎

这条命题给出了第 5.1 节“内在维度突变”的一个**可操作的充分条件**：一旦某层表示真的“忘掉了 $(a,b)$ 的二维自由度，只保留 $s$ 的一维自由度”，那么你用 PCA/TwoNN/局部维度估计看到的自由度下降就不是玄学，而是由表示因子分解强制的。

### 3.7 更贴近真实 LLM（GPT-4 类系统）的写法：连续近似 + 频谱/低复杂度偏置

如果要押注“现实中的 GPT-4 更像哪条数学路”，我会选：**连续近似（$S^1$ / 低维流形）+ 频谱/低复杂度偏置（spectral bias）**，而不是只在离散有限群里做完全严格的代数推导。原因很简单：真实 LLM 的训练数据与任务分布更接近“连续世界的采样”，而 Transformer 的表示通常表现出强烈的“先学低频/简单结构、后学高频/例外”的动力学偏置。

下面给出一个你可以直接写进论文、且不要求读者懂太多抽象代数的版本（它和第 3.6 节兼容，只是把“离散群 $\mathbb{Z}_p$”换成“连续圆周 $S^1$ 的近似”）。

**设定（把 $\mathbb{Z}_p$ 看成 $S^1$ 上的等间距采样）**  
把 $k\in\mathbb{Z}_p$ 映射到角度 $\theta_k = 2\pi k/p \in [0,2\pi)$，并把“和 $s$”对应到圆周位置 $\theta_s$。假设某层表示满足近似形式
$$
h(a,b)\approx \Phi(\theta_{(a+b)\bmod p}) \quad (\Phi:S^1\to\mathbb{R}^d\ \text{连续/平滑}).
$$
那么“发现结构”就等价于：网络在某层学到一个**低维参数化** $\theta\mapsto \Phi(\theta)$，而不是为每个 $(a,b)$ 记一套独立的表示。

**命题 3.5（圆周上的傅里叶展开：低频 = 平滑结构，高频 = 锯齿记忆）**  
对每个输出维度，设 $\Phi_j(\theta)$ 可平方可积，则存在傅里叶级数
$$
\Phi_j(\theta)=\sum_{m\in\mathbb{Z}} c_{j,m}\,e^{im\theta}.
$$
若系数在频率上快速衰减（例如 $\sum_m m^2 |c_{j,m}|^2 < \infty$），则 $\Phi$ 在 $\theta$ 上更平滑；反之，若需要大量高频分量才能拟合训练点的细碎差异，则对应“锯齿/记忆”式的表示。  
**说明**：这不是在“证明网络一定学低频”，而是在给出一个可量化的刻画：你可以在实验里对 $\theta$ 采样后的表示做离散傅里叶变换（DFT），看能量谱是否在 Grokking 前后从高频向低频集中。

**命题 3.6（平滑度正则的最小化会抑制高频：一个干净的变分结论）**  
考虑在 $S^1$ 上拟合目标函数 $g(\theta)$ 的变分问题：
$$
\min_{\Phi}\ \int_{S^1}\|\Phi(\theta)-g(\theta)\|^2\,d\theta \ +\ \alpha\int_{S^1}\|\partial_\theta \Phi(\theta)\|^2\,d\theta,\quad \alpha>0.
$$
则最优解在傅里叶域满足对频率 $m$ 的收缩：高频分量被更强地惩罚（因为 $\|\partial_\theta e^{im\theta}\|^2\propto m^2$），从而解更偏向低频/平滑结构。  
**说明**：真实网络里你并没有显式的 $\int\|\partial_\theta\Phi\|^2$ 正则，但“参数范数/权重衰减 + 优化动力学”经常表现出类似的低复杂度偏置；这给了你一条更贴近工程直觉的桥：**权重衰减 → 低复杂度偏置 → 高频受抑 → 表示更平滑 → 更像低维流形**。

把这一节放进来，你就能用“频谱能量从高频转向低频”去解释第 5 节的预测，并且它和真实 LLM 的观察（先学共性、后记例外）语言上更贴合。

---

## 4. 重新解释现有发现

### 4.1 为什么权重衰减太大/太小都不行

**太小**：向心力不够，模型稳定在锯齿曲线上，永远不探索流形。

**太大**：向心力太强，信号被压制，连锯齿曲线都画不出来，更别说发现流形。

**刚刚好**：锯齿曲线不稳定但信号够强，模型被迫探索，最终发现流形。

### 4.2 为什么数据量影响 Grokking

**数据太少**：流形上的采样点太稀疏，无法还原流形结构。模型只能画锯齿曲线。

**数据太多**：流形信号太强，模型直接发现流形，没有过拟合阶段，不存在"延迟泛化"。

**刚刚好**：流形信号存在但不明显，模型先画锯齿曲线，慢慢才发现曲线背后的结构。

这解释了为什么 Grokking 需要一个"Goldilocks"数据量。

### 4.3 为什么过参数化模型更容易 Grokking

**过参数化 = 表示空间足够大**

- 小模型：表示空间可能根本容不下真实流形
- 大模型：表示空间足够大，流形可以存在，只是需要时间去发现

**过参数化的悖论**：
- 传统观点：过参数化导致过拟合
- Grokking 视角：过参数化是泛化的**前提**，因为它提供了足够的表示空间

权重衰减解决了过参数化的"自由度过多"问题：虽然空间很大，但向心力把模型推向低维流形。

### 4.4 为什么 Grokking 是突然的（以及为什么会震荡）

**相变不是连续过渡**

流形发现是一个**拓扑事件**：

- 之前：高维锯齿曲线（记忆解）
- 之后：低维平滑流形（泛化解）

从高维到低维的坍缩没有稳定的中间状态，所以 Grokking 是突然的。

**类比：过冷水**

更准确的类比不是居里点（单向相变），而是**过冷水**——水可以在 0°C 以下保持液态，但随机扰动可以触发瞬间结冰，也可能再融化。

Grokking 类似：记忆解和泛化解是两个（伪）稳定态，权重衰减持续把模型推向临界点，优化噪声可以触发跃迁。这解释了实验中观察到的**临界态震荡**——模型可能在 100% → 0.8% → 100% 之间反复跳跃，直到最终稳定。

---

## 5. 可验证预测

这一节的写法我刻意不“完美闭环”。我们不承诺某条曲线**一定**长什么样，而是给出一组**可证伪的观测签名**：你做实验后，结果会把故事推向某个方向（支持 / 修正 / 推翻），而不是落入“怎么解释都对”的换词游戏。

### 5.1 内在维度突变

**观测签名**：Grokking 前后，中间层表示的内在维度（intrinsic dimension）可能出现“突变/坍缩”，也可能只出现“缓慢漂移”。

两种互斥的读数（给实验一个“二选一”的裁决权）：
- **若表示确实发生了因子化**（例如某层开始近似只依赖于同余类/群不变量），内在维度应从“接近样本自由度”显著下降到“接近任务自由度”（比如模加的 1D 结构）。
- **若泛化来自别的机制**（例如只是决策边界变得更稳，但表示未因子化），你可能看不到明显降维，只看到维度估计随训练平滑变化。

**实验设计**：
1. 训练模型做模运算，记录中间层激活
2. 用现成的算法（如 SVD、PCA 或 TwoNN）估计内在维度
3. 画出内在维度随训练步数的变化曲线

**判别点**：内在维度曲线是否在测试准确率跃迁附近出现结构性转折（突变或明显折点）。

### 5.2 表示的拓扑结构

**观测签名**：Grokking 后，中间层表示的拓扑结构可能开始“对上”任务结构，也可能只是出现弱相关。

- 模运算 $a + b \mod p$：表示应该形成一个一维的环
- 对称群任务：表示应该形成对应的群流形

**实验设计**：
1. 提取 Grokking 前后的中间层表示
2. 用持续同调（persistent homology）计算拓扑不变量
3. 比较与任务真实拓扑的匹配程度

**判别点**：Betti 数/持久条形图是否在“顿悟”附近出现清晰的拓扑签名（例如环结构的 $\beta_1$ 增强），且在不同随机种子下具有稳定性。

### 5.3 注意力熵的动态

**观测签名**：Grokking 过程中，attention pattern 的熵可能呈现“低→高→中”的探索-收敛过程，也可能完全不服从这个叙事（那就说明注意力并不是关键变量）。

给出两种可能的可分辨形态：
- **探索-收敛型**：早期低熵（专用模式）→ 临界前熵上升（模式多样化）→ 之后回落并稳定（共享结构）。
- **单调型/无关型**：熵单调变化或噪声很大、与测试跃迁时间无关——这会削弱“注意力坍缩”在该任务中的解释权重。

**实验设计**：
1. 记录每个训练步的 attention matrix
2. 计算 attention 分布的熵
3. 画出熵随训练步数的变化曲线

**判别点**：熵的峰/谷是否与测试跃迁对齐，并在多 seed 下重复出现。

### 5.4 强制秩约束的影响

**观测签名**：如果“表示维度/秩”真的是瓶颈，你对中间层施加低秩约束应该能系统性地改变 Grokking 的出现时间；如果几乎不变，那说明决定性因素可能不在“表示容量”而在别处（优化/数值稳定/归一化等）。

- 秩约束 = 任务真实自由度：加速 Grokking（直接告诉模型答案的维度）
- 秩约束 < 任务真实自由度：阻止 Grokking（表示空间装不下流形）
- 秩约束 > 任务真实自由度：不影响或轻微减慢

**实验设计**：
1. 在中间层添加低秩瓶颈（如线性层限制输出维度）
2. 改变瓶颈维度，记录 Grokking 时间
3. 与无瓶颈的 baseline 比较

**判别点**：Grokking 时间-瓶颈维度曲线是否存在明显“相变区间”（一旦低于某阈值就彻底失效），并且阈值是否与任务的最小自由度同量级。

---

## 6. 实验验证

我们设计了两组实验来验证流形发现假说：模加法（$a + b \mod 97$）和模乘法（$a \times b \mod 97$）。每组包含五个子实验：内在维度分析、拓扑结构分析、激活动态分析、瓶颈实验、流形可视化。

### 6.1 实验配置

| 参数 | 模加法 | 模乘法 |
|------|--------|--------|
| 任务 | $(a + b) \mod 97$ | $(a \times b) \mod 97$ |
| 数据集大小 | $97^2 = 9409$ 对 | $(97-1)^2 = 9216$ 对 |
| 训练集比例 | 30% | 30% |
| 模型 | 2 层 Transformer | 2 层 Transformer |
| Hidden dim | 128 | 128 |
| Attention heads | 4 | 4 |
| 优化器 | AdamW | AdamW |
| 学习率 | 1e-3 | 1e-3 |
| Weight decay | 1.0 | 1.0 |
| 总步数 | 150,000 | 150,000 |

**评估协议**：
- 输入编码：$(a, b)$ 作为两个 token，通过 embedding 层映射到 hidden_dim
- 输出：分类任务，模加法 97 类，模乘法 96 类
- Loss：标准交叉熵
- 测试集：70% 的数据（与训练集不重叠）
- 随机准确率：模加法 $1/97 \approx 1.03\%$，模乘法 $1/96 \approx 1.04\%$
- 文中 < 1% 的准确率均在随机水平附近

**数学背景**：
- 模加法对应加法群 $\mathbb{Z}_{97}$，97 个元素，循环群，输出 97 类
- 模乘法对应乘法群 $\mathbb{Z}_{97}^*$，96 个元素（排除 0），同构于 $\mathbb{Z}_{96}$（也是循环群），输出 96 类
- 两者群论结构相同（都是循环群），但模乘法在输入坐标 $(a, b)$ 下更非线性——加法只需要学 $(a+b) \mod p$，乘法需要学离散对数的加法 $(\log a + \log b) \mod (p-1)$

**分析方法参数**（可复现配置）：

| 分析类型 | 工具/库 | 关键参数 |
|----------|---------|----------|
| **持续同调** | ripser + persim | 点云子采样 500 点，欧氏距离，Vietoris-Rips 复形，maxdim=1，持久性阈值 0.1，random_state=42 |
| **UMAP 可视化** | umap-learn | n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42 |
| **邻接分析** | scipy.spatial.distance | 对簇中心做 k=2 最近邻，欧氏距离，簇中心=同 label 样本的 UMAP embedding 均值 |
| **激活提取** | PyTorch | 最后一层 Transformer 输出，两个 token 平均池化，得到 $n \times 128$ 矩阵 |

**注**：
- 持续同调的 Betti 数对子采样规模和阈值敏感，表中数值应理解为量级而非精确值
- UMAP 的拓扑保持性依赖参数选择，不同参数可能得到不同的视觉结构
- 邻接分析在 UMAP 降维后的 2D 空间进行，而非原始 128 维空间

### 6.2 实验组1：模加法结果

#### 6.2.1 内在维度突变 ✅

**方法说明**：
- 提取最后一层 Transformer 的输出激活（所有测试样本）
- 对每个样本取两个 token 的平均池化，得到 $n \times 128$ 的矩阵
- 使用 PCA，取解释 95% 累计方差的最小主成分数作为"有效维度"
- 注：这是"有效秩/能量维数"，依赖归一化方式，不等价于严格的内在维度估计（如 TwoNN）

使用 PCA（95% 方差解释阈值）估计内在维度：

| Step | PCA Dim | Test Acc | 状态 |
|------|---------|----------|------|
| 1000 | **78** | 0.1% | 初始（高维混乱） |
| 7000 | **8** | 24.9% | **最低点** |
| 9000 | 12 | **100%** | 首次 Grokking |
| 11000 | 3 | 8.6% | 崩溃 |
| 150000 | 13 | 96% | 最终稳定 |

**结论**：维度从 78 骤降到 8，**支持假说**。

#### 6.2.2 拓扑结构分析 ✅

使用持续同调（Persistent Homology）计算 Betti 数（step 7000 为 Grokking 前的维度最低点，step 14000 为首次崩溃后的恢复期）：

| 指标 | Before (step 7000) | After (step 14000) | 变化 |
|------|--------|-------|------|
| β₀ (连通分量) | 500 | 6 | **-99%** |
| β₁ (环) | 504 | 0 | **-100%** |
| β₀ 最大持久性 | 7.59 | 0.23 | -97% |
| β₁ 最大持久性 | 2.89 | 0.04 | -99% |

**结论**：在本文采用的拓扑管线与阈值下，观测到明显的"拓扑坍缩"趋势（连通分量数大幅下降、环结构显著消失），与"从碎片化记忆解走向更结构化表示"的叙事一致。

**⚠️ 关于 β₁=0 的重要说明**：Grokking 后 β₁ 从 504 降到 0，看似与"环结构"矛盾。但这恰好说明模型学到的是**按 label 分离的 97 个离散簇**，而非沿环连续分布的表示。在持续同调的视角下，97 个互不连通的簇不会形成环（β₁=0），但它们可以被理解为"离散采样的循环群"——每个簇对应一个同余类，簇之间的邻接关系（在 UMAP 可视化中）仍反映群结构。**这解释了为什么 6.6 节的邻接得分为 0%——模型学到了分类，没学到拓扑。**

#### 6.2.3 激活动态分析 ✅

每次准确率崩溃时，L2 范数和标准差同步暴跌：

| Step | Test Acc | L2 范数 | Std | 状态 |
|------|----------|---------|-----|------|
| 9000 | 100% | 9.9 | 0.88 | 正常 |
| 14000 | 0.8% | **1.79** | **0.16** | **崩溃** |
| 17000 | 98.6% | 9.7 | 0.87 | 恢复 |
| 26000 | 1% | **1.88** | **0.17** | **崩溃** |
| 54000 | 0.5% | **3.65** | **0.33** | **崩溃** |

**意外发现：临界态震荡**——模型在泛化解和"空白状态"之间反复跳跃（崩溃时 L2 从 ~10 骤降到 ~2），而非一次性相变。

#### 6.2.4 瓶颈实验 ✅

在中间层添加低秩瓶颈，测试维度下界：

| Bottleneck Dim | Final Acc | 状态 |
|----------------|-----------|------|
| 1 | 7.6% | ❌ 完全失败 |
| 2 | 20.9% | ❌ 失败 |
| 4 | 8.8% | ❌ 失败 |
| 8 | 1.3% | ❌ 失败 |
| 16 | 99.7% | ✅ 慢 Grok（2x） |
| 32 | 100% | ✅ 更慢（4x） |
| 64 | 85% | 🤔 不稳定 |
| 128 (baseline) | 100% | ✅ 正常 |

**结论**：临界点在 8-16 维之间。**支持假说**——存在维度下界。

#### 6.2.5 流形可视化 ✅

使用 UMAP 将 128 维表示降到 2D：

- **Step 5,000（记忆期）**：一团乱点，颜色混杂
- **Step 30,000（过渡期）**：97 个簇出现，每个 label 一个簇
- **Step 100,000（Grok 后）**：簇更紧致，结构稳定

**结论**：从乱点到按 label 分离的簇结构可直接观察，支持"表示在训练中发生了结构化重组"的主张。

### 6.3 实验组2：模乘法结果

#### 6.3.1 内在维度 ✅

| Step | PCA Dim | Test Acc |
|------|---------|----------|
| 1000 | **89** | 0.1% |
| 9000 | **11** | **100%** |
| 150000 | 17 | 100% |

**与模加法对比**：初始维度更高（89 vs 78），最低维度也更高（11 vs 8），符合乘法群结构更复杂的预期。

#### 6.3.2 拓扑结构

使用持续同调计算 Betti 数（与模加法相同时间点：step 7000 vs step 14000）：

| 指标 | Before (step 7000) | After (step 14000) | 变化 |
|------|--------|-------|------|
| β₀ | 500 | 500 | 无变化 |
| β₁ | 870 | 179 | **-79%** |
| β₁ 最大持久性 | 2.30 | 1.96 | -15% |

**意外发现**：模乘法没有出现 β₀ 坍缩（不像模加法从 500→6），但 β₁ 明显下降。这提示**两种运算的拓扑演化模式不同**。

#### 6.3.3 瓶颈实验

| Bottleneck Dim | Final Acc | 状态 |
|----------------|-----------|------|
| 1-8 | < 28% | ❌ 失败 |
| 16 | 98.2% | 🤔 接近但不稳定 |
| 32 | 99.9% | ✅ 慢 Grok |
| **64** | **29.3%** | ❌ **异常！** |
| 128 | 100% | ✅ 正常 |

**关键发现**：
1. 临界点更高（16-32 维 vs 8-16 维）
2. 64 维异常——两组实验都不稳定（模加法 85%，模乘法 29%），可能是某种共振区间

#### 6.3.4 流形可视化

- **Step 5,000**：两团乱点（与模加法的一团不同）
- **Step 30,000**：96 个点排成对角线
- **Step 100,000**：**~12 个大簇**（颜色混合）

**关键问题**：为什么是 12 个簇而不是 96 个？

### 6.4 陪集结构验证：12 簇 = $k \mod 12$

我们验证了 12 个簇对应乘法群的陪集结构：

**方法**：
1. 找 97 的原根 $g = 5$
2. 对每个 label $y$，计算离散对数 $k$ 使得 $5^k \equiv y \pmod{97}$
3. 用 KMeans 聚类，检验每个簇的 $k \mod 12$ 分布

**结果**：

| 簇 | 主要 k mod 12 | 纯度 |
|---|---|---|
| 0 | 11 | 99.64% |
| 1 | 4 | 99.81% |
| ... | ... | ... |
| 10 | 2 | **100.00%** |

**平均纯度：99.4%**

**结论**：模型学到了商群坐标 $k \mod 12$（即陪集标识），而非完整的离散对数 $k$。这意味着：
- 通过离散对数同构 $\mathbb{Z}_{97}^* \cong \mathbb{Z}_{96}$，乘法群被映射到加法群
- 在 $\mathbb{Z}_{96}$ 上，$k \mod 12$ 对应商群 $\mathbb{Z}_{96} / 12\mathbb{Z}_{96} \cong \mathbb{Z}_{12}$
- 12 个簇 = 8 阶子群 $H = 12\mathbb{Z}_{96}$ 的 12 个陪集
- **局部流形发现完成，全局粘合未完成**

### 6.5 假说修正：两阶段模型

基于实验结果，我们将原假说修正为**两阶段模型**：

| 阶段 | 现象 | 指标变化 | 模加法 | 模乘法 |
|------|------|----------|--------|--------|
| 1. 局部流形发现 | 分量内部结构化 | β₁ 降、维度降 | ✅ 完成 | ✅ 完成 |
| 2. 全局粘合 | 分量之间对齐 | β₀ 降 | ✅ 完成 | ❌ 未完成 |

**解释**：
- 模加法：两阶段都完成，β₀ 从 500 降到 6
- 模乘法：只完成第一阶段，β₀ 保持 500，但学到了商群结构

**直觉表述**（可选，偏叙事风格）：
> Grokking = 熵的相变 + 结构的结晶
> - 加法 = 液化（连续圆环）
> - 乘法 = 结晶（子群晶体）

### 6.6 邻接关系分析：拓扑保持的差异

我们进一步验证模型是否学到了群的拓扑结构（邻接关系），还是只学到了离散的等价类分类。

**模加法**：检查 97 个簇的最近邻是否为 $s \pm 1 \mod 97$
- 邻接得分：**0%**（随机基线 2.1%）
- 结论：❌ 未观察到环形邻接被保留的证据

**模乘法**：检查 12 个陪集的最近邻是否为 $(k \pm 1) \mod 12$
- 邻接得分：**100%**（随机基线 16.7%）
- 所有 12 个陪集的最近邻都是 $k \pm 1$
- 结论：✅ 模型完美学到了 $\mathbb{Z}_{12}$ 的环形拓扑

**对比**：

| 指标 | 模加法 | 模乘法 |
|------|--------|--------|
| 邻接得分 | 0% | **100%** |
| 随机基线 | 2.1% | 16.7% |
| 结构类型 | 离散等价类 | 完美 $\mathbb{Z}_{12}$ 环 |

**核心洞见**：乘法群的商结构（$\mathbb{Z}_{12}$）在嵌入空间中保持了拓扑完整性，而加法群的循环结构（$\mathbb{Z}_{97}$）被打散成离散点。可能原因：97 个点太多，模型没有足够动机保持邻接；12 个陪集正好在"可处理"的规模。

### 6.7 多 Seed 稳定性验证

我们用 3 个额外 seed（1001, 1002, 1003）验证核心发现的可复现性。

**模加法**：

| Seed | 首次 Grok | 震荡次数 | 最终准确率 |
|------|-----------|----------|------------|
| 1001 | 9000 | 13 | **100%** ✅ |
| 1002 | 10000 | 20 | **29.1%** ❌ |
| 1003 | 9000 | 16 | **100%** ✅ |
| **均值** | 9333 ± 471 | 16.3 ± 2.9 | 2/3 成功 |

**模乘法**：

| Seed | 首次 Grok | 震荡次数 | 最终准确率 |
|------|-----------|----------|------------|
| 1001 | 13000 | 14 | **33.3%** ❌ |
| 1002 | 10000 | 17 | **100%** ✅ |
| 1003 | 10000 | 12 | **98.9%** ✅ |
| **均值** | 11000 ± 1414 | 14.3 ± 2.1 | 2/3 成功 |

**核心发现**：

1. **Grokking 成功率 ~67%**：两组实验都是 2/3 成功，相变不是必然的
2. **震荡是普遍现象**：所有 seed 都有 12-20 次震荡，临界态竞争实锤
3. **失败模式一致**：失败的 seed 最终准确率 ≈ 30%（训练集比例），说明卡在记忆解

**修正后的理论叙事**：
> Grokking = 高维曲线 → 震荡 → 低维流形
> 但相变不是必然成功，有 ~1/3 概率卡死在记忆解。

### 6.8 两组实验的跨运算对比

| 指标 | 模加法 | 模乘法 | 结论 |
|------|--------|--------|------|
| 维度变化 | 78 → 8 | 89 → 11 | 都骤降 ✅ |
| 首次 Grok | 9333 ± 471 步 | 11000 ± 1414 步 | 乘法略慢 |
| 震荡次数 | 16.3 ± 2.9 | 14.3 ± 2.1 | 都有震荡 |
| 成功率 | 67% (2/3) | 67% (2/3) | 相变是概率性的 |
| 拓扑变化 | β₀ 500→6, β₁ 504→0 | β₀ 不变, β₁ 870→179 | 模式不同 |
| 邻接得分 | 0% | 100% | 乘法保持拓扑 |
| 瓶颈临界点 | 8-16 维 | 16-32 维 | 乘法需要更多维度 |
| 最终结构 | 97 个簇 | 12 个陪集 | 加法=完整群，乘法=商群 |
| 64 维异常 | 85% | 29% | 两组都不稳定 |

**核心结论**：**流形发现假说在两种运算上都得到验证**，但发现的"深度"和"质量"不同——模加法发现了完整的循环群结构但丢失了拓扑邻接，模乘法只发现了商群结构但完美保持了环形拓扑。

---

## 7. 讨论

### 7.1 现有研究的方法论局限

现有 Grokking 研究的测量对象：
- 权重范数
- 梯度大小
- Loss 曲线
- 测试准确率

这些全是**外部观测量**——从模型外部可以测量到的数值。

**类比**：研究人类学习，只测量脑电波和瞳孔直径，不问"学会了是什么感觉"。

这种方法论能回答"什么条件下 Grokking 发生"，不能回答"Grokking 是什么"。

### 7.2 内部视角的价值

本文提出的流形发现假说，是一个**内部视角**的解释：

- 不问"权重范数在哪个区间"
- 问"表示空间的结构是什么"

这个视角的价值：
1. **统一性**：能同时解释 Goldilocks Zone、Softmax Collapse、Lazy→Rich，并与 Nanda et al. 的 circuit 发现兼容
2. **可预测性**：生成可验证的实验预测（内在维度、拓扑结构等）
3. **启发性**：指向新的研究方向（直接测量表示结构的几何性质，而不仅是逆向工程具体电路）

### 7.3 实验发现的理论意义

本文的实验验证带来了几个重要的理论洞见：

**1. 临界态震荡**

原假说预测 Grokking 是一次性的拓扑相变。但实验观察到模型在泛化解和"空白状态"之间反复跳跃（准确率 100% → 0.8% → 100% → ...）。这提示 Grokking 更像"过冷水"——在冰和水之间不稳定，随机扰动可以触发结晶或融化。

**2. 64 维异常**

两组实验中 64 维瓶颈都表现异常（模加法 85%，模乘法 29%）。一种解释是 64 维处于"过完备但未对齐"的临界状态——太小逼你 Grok，太大让你 Memorize，中间是精神分裂。

**3. 商群结构的发现**

模乘法实验中模型只学到了 $k \mod 12$（陪集结构），而非完整的离散对数。这说明：
- 流形发现可以是"部分的"——先发现商群，再（可能）发现完整群
- 这与两阶段模型一致：局部流形发现 → 全局粘合

**4. 维度下界与任务复杂度**

模加法临界点 8-16 维，模乘法 16-32 维。这符合直觉：乘法群结构更复杂（有非平凡子群），需要更多维度来编码。

### 7.4 局限与未来方向

本文的局限：
1. **任务范围有限**：只验证了模加法和模乘法，不清楚是否适用于其他 Grokking 任务（如排列群、多项式求值）
2. **两阶段模型待验证**：模乘法的"全局粘合"是否会在更长训练后发生？
3. **64 维异常待解释**：目前只有直觉解释，缺乏严格理论

未来方向：
1. 在更多任务上验证两阶段模型
2. 探索是否可以通过操控表示空间拓扑来控制 Grokking
3. 研究 64 维异常的数学机制
4. 验证延长训练是否能让模乘法完成"全局粘合"

---

## 8. 结论

Grokking 不是一个反常现象，而是深度学习的一扇窗户——它揭示了泛化的本质。

本文提出的流形发现假说提供了一个内部视角的统一框架：

- **记忆 = 锯齿曲线**（高维、按样本编码）
- **泛化 = 平滑流形**（低维、按结构编码）
- **Grokking = 多稳态系统的相变**（可能伴随临界态震荡）
- **权重衰减 = 让锯齿曲线不稳定的向心力**

**我们在两组实验上验证了这一假说**：

| 指标 | 模加法 | 模乘法 |
|------|--------|--------|
| 维度变化 | 78 → 8 | 89 → 11 |
| 拓扑变化 | β₀ 500→6, β₁ 504→0 | β₀ 不变, β₁ 870→179 |
| 瓶颈临界点 | 8-16 维 | 16-32 维 |
| 流形结构 | 97 个簇 | 12 个陪集（纯度 99.4%） |
| 邻接得分 | 0%（无环拓扑） | 100%（完美 $\mathbb{Z}_{12}$ 环） |
| 多 seed 成功率 | 67% (2/3) | 67% (2/3) |
| 震荡次数 | 16.3 ± 2.9 | 14.3 ± 2.1 |

基于实验发现，我们将假说修正为**两阶段模型**：
1. **局部流形发现**：分量内部结构化（维度降、β₁ 降）
2. **全局粘合**：分量之间对齐（β₀ 降）

模加法完成了两阶段，模乘法只完成了第一阶段——这解释了为什么模乘法学到的是商群结构而非完整的乘法群。

**一句话概括：高维曲线 → 震荡 → 低维曲面（成功率 ~67%，可能分两步走）。**

---

## 参考文献

1. Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. arXiv:2201.02177.

2. Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark, M., & Williams, M. (2022). Towards Understanding Grokking: An Effective Theory of Representation Learning. NeurIPS 2022. arXiv:2205.10343.

3. Liu, Z., Michaud, E. J., & Tegmark, M. (2023). Omnigrok: Grokking Beyond Algorithmic Data. ICLR 2023. arXiv:2210.01117.

4. Prieto, L., Barsbey, M., Mediano, P. A. M., & Birdal, T. (2025). Grokking at the Edge of Numerical Stability. ICLR 2025. arXiv:2501.04697.

5. Kumar, T., Bordelon, B., Gershman, S. J., & Pehlevan, C. (2024). Grokking as the Transition from Lazy to Rich Training Dynamics. ICLR 2024. arXiv:2310.06110.

6. Varma, V., Shah, R., Kenton, Z., Kramár, J., & Kumar, R. (2023). Explaining Grokking Through Circuit Efficiency. arXiv:2309.02390.

7. Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2023). Progress Measures for Grokking via Mechanistic Interpretability. ICLR 2023 (Oral). arXiv:2301.05217.

8. Facco, E., d'Errico, M., Rodriguez, A., & Laio, A. (2017). Estimating the intrinsic dimension of datasets by a minimal neighborhood information. Scientific Reports, 7(1), 12140.

9. Carlsson, G. (2009). Topology and data. Bulletin of the American Mathematical Society, 46(2), 255-308.

---

*"现有研究主要在问'什么条件下 Grokking 发生'，本文尝试追问'Grokking 是什么'。前者是工程问题，后者是本体论问题。"*
